{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow torch opencv-python pandas scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define the path to the saved model file\n",
    "model_save_dir = 'saved_ocr_model'\n",
    "model_save_path = os.path.join(model_save_dir, 'combined_ocr_model.keras')\n",
    "\n",
    "# Check if the model file exists before attempting to load\n",
    "if os.path.exists(model_save_path):\n",
    "    print(f\"Loading the model from: {model_save_path}\")\n",
    "    try:\n",
    "        # Load the model\n",
    "        loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        # You can optionally print a summary of the loaded model to verify\n",
    "        # loaded_model.summary()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "        loaded_model = None # Set to None if loading fails\n",
    "else:\n",
    "    print(f\"Error: Model file not found at {model_save_path}. Please ensure the model has been saved.\")\n",
    "    loaded_model = None # Set to None if file not found\n",
    "\n",
    "# Now you can use the 'loaded_model' for predictions\n",
    "if loaded_model is not None:\n",
    "    print(\"\\nThe loaded model is ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the test CSV file\n",
    "test_csv_file_path = 'extracted_archive/testdata.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.exists(test_csv_file_path):\n",
    "    print(f\"Error: The file {test_csv_file_path} was not found.\")\n",
    "else:\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        # Assuming the file is comma-separated and has a header row\n",
    "        # We'll explicitly name the columns as they might not be consistently named\n",
    "        test_df = pd.read_csv(test_csv_file_path, sep=',', header=0, names=['ImgName', 'GroundTruth', 'smallLexi', 'mediumLexi'])\n",
    "\n",
    "        # --- Filter DataFrame to include only 'test' images ---\n",
    "        initial_rows_test = len(test_df)\n",
    "        test_df = test_df[test_df['ImgName'].str.startswith('test/')]\n",
    "        filtered_rows_test = len(test_df)\n",
    "        print(f\"Filtered test data: Kept {filtered_rows_test} rows starting with 'test/' out of {initial_rows_test}.\")\n",
    "        # --- End filtering ---\n",
    "\n",
    "\n",
    "        # Print the first few rows of the DataFrame\n",
    "        print(\"Test CSV data loaded successfully:\")\n",
    "        display(test_df.head())\n",
    "\n",
    "        # Print DataFrame information\n",
    "        print(\"\\nTest DataFrame Info:\")\n",
    "        test_df.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the test CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume resize_image, normalize_pixels, and grayscale_image functions are already defined\n",
    "# Assume test_df is loaded from the previous step\n",
    "\n",
    "# 1. Create empty lists\n",
    "processed_test_images = []\n",
    "original_test_labels = []\n",
    "\n",
    "# Correct Base directory for images - same as training data\n",
    "base_image_dir = 'extracted_archive/IIIT5K-Word_V3.0/IIIT5K'\n",
    "\n",
    "# Target size for resizing (same as training)\n",
    "target_size = (128, 32) # (width, height)\n",
    "\n",
    "# Check if test_df exists and is not empty before proceeding\n",
    "if 'test_df' in locals() and not test_df.empty:\n",
    "    # 2. Iterate through each row of the test_df DataFrame\n",
    "    for index, row in test_df.iterrows():\n",
    "        # 3. Get image path and text label\n",
    "        image_path_relative = row['ImgName']\n",
    "        text_label = row['GroundTruth']\n",
    "\n",
    "        # Construct the full image path\n",
    "        full_image_path = os.path.join(base_image_dir, image_path_relative)\n",
    "\n",
    "        # 4. Check if the image file exists\n",
    "        if os.path.exists(full_image_path):\n",
    "            # 5. Read the image\n",
    "            img = cv2.imread(full_image_path)\n",
    "\n",
    "            # 6. If the image is successfully loaded (not None)\n",
    "            if img is not None:\n",
    "                # Apply preprocessing steps\n",
    "                gray_img = grayscale_image(img)\n",
    "                resized_img = resize_image(gray_img, target_size)\n",
    "                normalized_img = normalize_pixels(resized_img) # This is float32\n",
    "\n",
    "                # 7. Append the preprocessed image (float32) to the list\n",
    "                processed_test_images.append(normalized_img)\n",
    "\n",
    "                # 8. Append the original text label\n",
    "                original_test_labels.append(text_label)\n",
    "            else:\n",
    "                # 9. If image cannot be loaded, print warning and skip\n",
    "                print(f\"Warning: Could not load image file: {full_image_path}\")\n",
    "        else:\n",
    "            # 9. If image file not found, print warning and skip\n",
    "            print(f\"Warning: Image file not found: {full_image_path}\")\n",
    "\n",
    "    # 10. Convert the processed_test_images list into a NumPy array\n",
    "    processed_test_images = np.array(processed_test_images)\n",
    "\n",
    "    # 11. Convert the original_test_labels list into a pandas Series or NumPy array\n",
    "    original_test_labels = pd.Series(original_test_labels)\n",
    "\n",
    "    # 12. Print the shape and length to verify\n",
    "    print(\"Preprocessing of test images complete.\")\n",
    "    print(\"Shape of processed_test_images array:\", processed_test_images.shape)\n",
    "    print(\"Length of original_test_labels list:\", len(original_test_labels))\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame 'test_df' not found or is empty. Please run the cell to load the test CSV first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK data ('averaged_perceptron_tagger', 'punkt') downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")\n",
    "\n",
    "\n",
    "def calculate_cer(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Calculates the Character Error Rate (CER) between two strings.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: The ground truth string.\n",
    "        prediction: The predicted string.\n",
    "\n",
    "    Returns:\n",
    "        The Character Error Rate (float). Returns 0 if ground_truth is empty.\n",
    "    \"\"\"\n",
    "    # Handle empty ground truth to avoid division by zero\n",
    "    if len(ground_truth) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate Levenshtein distance (character-level)\n",
    "    levenstein_dist = edit_distance(ground_truth, prediction)\n",
    "\n",
    "    # CER is Levenshtein distance divided by the length of the ground truth\n",
    "    cer = levenstein_dist / len(ground_truth)\n",
    "    return cer\n",
    "\n",
    "def calculate_wer(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Calculates the Word Error Rate (WER) between two strings.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: The ground truth string.\n",
    "        prediction: The predicted string.\n",
    "\n",
    "    Returns:\n",
    "        The Word Error Rate (float). Returns 0 if ground_truth is empty (after splitting into words).\n",
    "    \"\"\"\n",
    "    # Split strings into words\n",
    "    # Use a simple split by space for word tokenization\n",
    "    ground_truth_words = ground_truth.split()\n",
    "    prediction_words = prediction.split()\n",
    "\n",
    "    # Handle empty ground truth word list to avoid division by zero\n",
    "    if len(ground_truth_words) == 0:\n",
    "        # If ground truth is an empty string, WER should arguably be 0\n",
    "        # If prediction is also empty, error is 0. If prediction is not empty, error is high.\n",
    "        # A common approach for empty reference is to return 0 if hypothesis is also empty, else inf or 1.\n",
    "        # Let's return 0 for consistency with CER on empty string.\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    # Calculate Levenshtein distance (word-level)\n",
    "    # edit_distance can work on lists\n",
    "    levenstein_dist = edit_distance(ground_truth_words, prediction_words)\n",
    "\n",
    "\n",
    "    # WER is Levenshtein distance divided by the number of words in the ground truth\n",
    "    wer = levenstein_dist / len(ground_truth_words)\n",
    "    return wer\n",
    "\n",
    "print(\"Character Error Rate (CER) and Word Error Rate (WER) calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Redefine the decode_predictions function using a greedy approach\n",
    "def decode_predictions_greedy(predictions, int_to_char):\n",
    "    \"\"\"\n",
    "    Decodes the model's output predictions (probability distributions) into text sequences\n",
    "    using a greedy approach (taking the argmax at each time step) and handling blank tokens.\n",
    "\n",
    "    Args:\n",
    "        predictions: A TensorFlow Tensor of shape (time_steps, num_classes) or\n",
    "                     (batch_size, time_steps, num_classes) representing the predicted probabilities.\n",
    "        int_to_char: A dictionary mapping integer indices to characters.\n",
    "\n",
    "    Returns:\n",
    "        A list of decoded text strings (if input was batched) or a single string.\n",
    "    \"\"\"\n",
    "    if len(predictions.shape) == 2: # Single sample (time_steps, num_classes)\n",
    "        predictions = tf.expand_dims(predictions, axis=0) # Add batch dimension\n",
    "\n",
    "    # Get the index of the most probable character at each time step\n",
    "    predicted_indices = tf.argmax(predictions, axis=-1, output_type=tf.int32) # Shape (batch_size, time_steps)\n",
    "\n",
    "    # Find the integer value for the blank token\n",
    "    blank_int = len(int_to_char) - 1\n",
    "\n",
    "    decoded_texts = []\n",
    "    # Iterate through each sample in the batch\n",
    "    for sample_indices in predicted_indices.numpy(): # Convert tensor to NumPy array\n",
    "        decoded_sequence = []\n",
    "        # Iterate through the predicted indices for this sample\n",
    "        last_added = -1 # To handle CTC repeated character rule\n",
    "\n",
    "        for index in sample_indices:\n",
    "            # Check if the current index is a blank token\n",
    "            if index == blank_int:\n",
    "                last_added = blank_int # Remember that the last token was blank\n",
    "            else:\n",
    "                # If the current index is different from the last added index\n",
    "                # and the last added index was not a blank token, or if the last added was blank\n",
    "                if index != last_added or last_added == blank_int:\n",
    "                    decoded_sequence.append(int_to_char[index])\n",
    "                last_added = index # Update last added index\n",
    "\n",
    "        # Join characters to form the decoded string\n",
    "        decoded_text = \"\".join(decoded_sequence)\n",
    "        decoded_texts.append(decoded_text)\n",
    "\n",
    "    # If the input was a single sample, return a single string\n",
    "    if tf.shape(predictions)[0] == 1:\n",
    "        return decoded_texts[0]\n",
    "    else:\n",
    "        return decoded_texts\n",
    "\n",
    "print(\"Greedy decode predictions function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random # Import random to select random samples\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow # For displaying the image with regions\n",
    "\n",
    "# Assume test_df is available from previous steps (loaded testdata.csv)\n",
    "# Assume combined_model is the trained model (from the previous training loop)\n",
    "# Assume decode_predictions_greedy and int_to_char are defined (cell 79728814)\n",
    "# Assume calculate_cer and calculate_wer functions are defined (cell kOLluGXQ8bLg)\n",
    "# Assume grayscale_image, resize_image, and normalize_pixels functions are defined (cell 4db0fd6b)\n",
    "# Assume target_size = (128, 32) is defined\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the base directory for images (same as used for loading test data)\n",
    "base_image_dir = 'extracted_archive/IIIT5K-Word_V3.0/IIIT5K'\n",
    "\n",
    "# Number of samples to visualize\n",
    "num_samples_to_visualize = 10 # You can change this number\n",
    "\n",
    "# --- Evaluate Model on Test Set ---\n",
    "print(\"Starting evaluation on test set (sample by sample) with greedy decoding...\")\n",
    "\n",
    "# Convert the processed_test_images NumPy array to a TensorFlow Tensor\n",
    "# Ensure processed_test_images is in the correct shape (num_samples, height, width, 1)\n",
    "# and dtype (float32)\n",
    "# Assume processed_test_images is available from cell 7dfe8ff3\n",
    "if 'processed_test_images' in locals() and processed_test_images.size > 0:\n",
    "    if len(processed_test_images.shape) == 3: # Check if it's (num_samples, height, width)\n",
    "        processed_test_images = np.expand_dims(processed_test_images, axis=-1) # Add channel dimension\n",
    "\n",
    "    test_images_tensor = tf.constant(processed_test_images, dtype=tf.float32)\n",
    "\n",
    "    # Initialize empty lists to store the calculated CER and WER\n",
    "    all_cer = []\n",
    "    all_wer = []\n",
    "\n",
    "    total_samples = test_images_tensor.shape[0]\n",
    "    print(f\"Processing {total_samples} test samples...\")\n",
    "\n",
    "    # Process test data sample by sample to avoid batching issues with decoding\n",
    "    # Keep track of the current index\n",
    "    current_index = 0\n",
    "\n",
    "    for i in range(total_samples):\n",
    "        # Get a single image tensor\n",
    "        image_tensor = tf.expand_dims(test_images_tensor[i], axis=0) # Add batch dimension for prediction\n",
    "\n",
    "        # Get the model's prediction for the single image\n",
    "        # Predict will return shape (1, time_steps, num_classes)\n",
    "        prediction_single = combined_model.predict(image_tensor, verbose=0)\n",
    "\n",
    "        # Decode the single prediction using the greedy function\n",
    "        # Pass the prediction without the batch dimension for easier handling in the greedy function\n",
    "        decoded_text_single = decode_predictions_greedy(prediction_single[0], int_to_char)\n",
    "\n",
    "        # Get the corresponding ground truth label (from the original test_df)\n",
    "        # Assume original_test_labels is available from cell 7dfe8ff3\n",
    "        ground_truth_label = original_test_labels[i]\n",
    "\n",
    "        # Convert ground truth and predicted labels to lowercase for case-insensitive evaluation\n",
    "        ground_truth_lower = ground_truth_label.lower()\n",
    "        prediction_lower = decoded_text_single.lower()\n",
    "\n",
    "        # Calculate CER and WER\n",
    "        cer = calculate_cer(ground_truth_lower, prediction_lower)\n",
    "        wer = calculate_wer(ground_truth_lower, prediction_lower)\n",
    "\n",
    "        # Append to lists\n",
    "        all_cer.append(cer)\n",
    "        all_wer.append(wer)\n",
    "\n",
    "        current_index += 1\n",
    "\n",
    "        if current_index % 100 == 0:\n",
    "            print(f\"Processed {current_index}/{total_samples} samples.\")\n",
    "\n",
    "    # Calculate the average CER and WER\n",
    "    average_cer = np.mean(all_cer)\n",
    "    average_wer = np.mean(all_wer)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Average Character Error Rate (CER): {average_cer:.4f}\")\n",
    "    print(f\"Average Word Error Rate (WER): {average_wer:.4f}\")\n",
    "\n",
    "    print(\"Evaluation completed.\")\n",
    "\n",
    "    # --- Visualize Predictions on Random Samples ---\n",
    "    # Check if test_df exists and is not empty for visualization\n",
    "    if 'test_df' in locals() and not test_df.empty:\n",
    "        # Get a list of all possible indices in the test DataFrame\n",
    "        all_indices = list(test_df.index)\n",
    "\n",
    "        # Select random indices for visualization\n",
    "        if len(all_indices) >= num_samples_to_visualize:\n",
    "            sample_indices = random.sample(all_indices, num_samples_to_visualize)\n",
    "        else:\n",
    "            # If not enough samples, visualize all available\n",
    "            sample_indices = all_indices\n",
    "            num_samples_to_visualize = len(all_indices) # Update the number to visualize\n",
    "\n",
    "        print(f\"\\nVisualizing predictions for {num_samples_to_visualize} random samples from the test set.\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "        # Process and Visualize Samples\n",
    "        for i in sample_indices:\n",
    "            # Get the image path and ground truth label for the selected index\n",
    "            row = test_df.iloc[i]\n",
    "            image_path_relative = row['ImgName']\n",
    "            ground_truth_label = row['GroundTruth']\n",
    "\n",
    "            # Construct the full image path\n",
    "            full_image_path = os.path.join(base_image_dir, image_path_relative)\n",
    "\n",
    "            # Load the original image for display\n",
    "            original_image = cv2.imread(full_image_path)\n",
    "\n",
    "            if original_image is not None:\n",
    "                # Preprocess the image for the model (using defined functions)\n",
    "                gray_img = grayscale_image(original_image)\n",
    "                resized_img = resize_image(gray_img, target_size)\n",
    "                normalized_img = normalize_pixels(resized_img) # float32\n",
    "                # Add channel and batch dimensions\n",
    "                preprocessed_img_tensor = tf.constant(np.expand_dims(np.expand_dims(normalized_img, axis=-1), axis=0), dtype=tf.float32)\n",
    "\n",
    "                # Get model prediction\n",
    "                model_prediction = combined_model.predict(preprocessed_img_tensor, verbose=0)\n",
    "\n",
    "                # Decode the prediction using the greedy decoding function\n",
    "                decoded_text = decode_predictions_greedy(model_prediction[0], int_to_char)\n",
    "\n",
    "                # --- Display Results ---\n",
    "                print(f\"Sample Index: {i}\")\n",
    "                print(f\"Ground Truth: {ground_truth_label}\")\n",
    "                print(f\"Model Prediction: {decoded_text}\")\n",
    "\n",
    "                # Display the original image (resized for consistent size)\n",
    "                display_original_img = cv2.resize(original_image, (200, 50)) # Example resize\n",
    "                cv2_imshow(display_original_img)\n",
    "\n",
    "            else:\n",
    "                print(f\"Warning: Could not load image file for visualization: {full_image_path}\")\n",
    "\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        print(\"Visualization complete.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nDataFrame 'test_df' not found or is empty. Skipping visualization.\")\n",
    "\n",
    "else:\n",
    "    print(\"NumPy array 'processed_test_images' not found or is empty. Skipping evaluation and visualization.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
