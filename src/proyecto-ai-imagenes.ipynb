{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow torch opencv-python pandas scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "csv_file_path = 'extracted_archive/traindata.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Error: The file {csv_file_path} was not found.\")\n",
    "else:\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        # The file seems to be comma-separated based on the name and previous attempts\n",
    "        # Assuming the first row is the header based on the visual inspection of the error output\n",
    "        # We'll explicitly name the columns as they might not be consistently named\n",
    "        df = pd.read_csv(csv_file_path, sep=',', header=0, names=['ImgName', 'GroundTruth', 'smallLexi', 'mediumLexi'])\n",
    "\n",
    "        # --- Filter DataFrame to include only 'train' images ---\n",
    "        initial_rows = len(df)\n",
    "        df = df[df['ImgName'].str.startswith('train/')]\n",
    "        filtered_rows = len(df)\n",
    "        print(f\"Filtered training data: Kept {filtered_rows} rows starting with 'train/' out of {initial_rows}.\")\n",
    "        # --- End filtering ---\n",
    "\n",
    "        # Display the first few rows of the DataFrame to verify\n",
    "        print(\"CSV data loaded successfully:\")\n",
    "        display(df.head())\n",
    "\n",
    "        # You can now access image paths and text labels using df['ImgName'] and df['GroundTruth']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    \"\"\"\n",
    "    Resizes an image to a target size.\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array).\n",
    "        target_size: A tuple (width, height) representing the desired size.\n",
    "\n",
    "    Returns:\n",
    "        The resized image (NumPy array).\n",
    "    \"\"\"\n",
    "    return cv2.resize(image, target_size)\n",
    "\n",
    "def normalize_pixels(image):\n",
    "    \"\"\"\n",
    "    Normalizes pixel values of an image to the range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        The normalized image (NumPy array).\n",
    "    \"\"\"\n",
    "    # Convert to float for accurate division\n",
    "    return image.astype('float32') / 255.0\n",
    "\n",
    "def grayscale_image(image):\n",
    "    \"\"\"\n",
    "    Converts an image to grayscale.\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        The grayscale image (NumPy array).\n",
    "    \"\"\"\n",
    "    # Check if the image is already grayscale\n",
    "    if len(image.shape) == 2:\n",
    "        return image\n",
    "    # Convert to grayscale\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(\"Preprocessing functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def augment_image_cv2(image):\n",
    "    \"\"\"\n",
    "    Applies a series of random data augmentation techniques to an image using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        The augmented image (NumPy array).\n",
    "    \"\"\"\n",
    "    augmented_image = image.copy()\n",
    "    h, w = augmented_image.shape[:2]\n",
    "\n",
    "    # Random Rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-5, 5)\n",
    "        M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Translation\n",
    "    if random.random() < 0.5:\n",
    "        tx = random.uniform(-0.05, 0.05) * w\n",
    "        ty = random.uniform(-0.05, 0.05) * h\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Scaling\n",
    "    if random.random() < 0.5:\n",
    "        sx = random.uniform(0.95, 1.05)\n",
    "        sy = random.uniform(0.95, 1.05)\n",
    "        M = np.float32([[sx, 0, 0], [0, sy, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Shear (simplified)\n",
    "    if random.random() < 0.5:\n",
    "        shear_factor = random.uniform(-0.05, 0.05)\n",
    "        M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "    # Random Gaussian Blur\n",
    "    if random.random() < 0.25:\n",
    "        sigma = random.uniform(0, 1.0)\n",
    "        ksize = int(2 * int(sigma * 3 + 0.5) + 1) # Kernel size based on sigma\n",
    "        ksize = ksize if ksize % 2 == 1 else ksize + 1 # Ensure kernel size is odd\n",
    "        if ksize > 1: # Apply blur only if kernel size is greater than 1\n",
    "          augmented_image = cv2.GaussianBlur(augmented_image, (ksize, ksize), sigma)\n",
    "\n",
    "\n",
    "    # Random Gaussian Noise\n",
    "    if random.random() < 0.25:\n",
    "        mean = 0\n",
    "        stddev = random.uniform(0, 0.05 * 255)\n",
    "        noise = np.random.normal(mean, stddev, augmented_image.shape).astype(np.uint8)\n",
    "        augmented_image = cv2.add(augmented_image, noise)\n",
    "\n",
    "\n",
    "    # Random Brightness and Contrast\n",
    "    if random.random() < 0.25:\n",
    "        alpha = random.uniform(0.9, 1.1) # Contrast control (1.0-base contrast)\n",
    "        beta = random.uniform(-10, 10) # Brightness control (0-base brightness)\n",
    "        augmented_image = cv2.convertScaleAbs(augmented_image, alpha=alpha, beta=beta)\n",
    "\n",
    "    # Random Grayscale (only if image is not already grayscale)\n",
    "    if random.random() < 0.25 and len(augmented_image.shape) == 3:\n",
    "         augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    return augmented_image.astype(np.uint8)\n",
    "\n",
    "print(\"Data augmentation function defined using OpenCV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def augment_image_cv2(image):\n",
    "    \"\"\"\n",
    "    Applies a series of random data augmentation techniques to an image using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array).\n",
    "\n",
    "    Returns:\n",
    "        The augmented image (NumPy array).\n",
    "    \"\"\"\n",
    "    augmented_image = image.copy()\n",
    "    h, w = augmented_image.shape[:2]\n",
    "\n",
    "    # Random Rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-5, 5)\n",
    "        M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Translation\n",
    "    if random.random() < 0.5:\n",
    "        tx = random.uniform(-0.05, 0.05) * w\n",
    "        ty = random.uniform(-0.05, 0.05) * h\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Scaling\n",
    "    if random.random() < 0.5:\n",
    "        sx = random.uniform(0.95, 1.05)\n",
    "        sy = random.uniform(0.95, 1.05)\n",
    "        M = np.float32([[sx, 0, 0], [0, sy, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Shear (simplified)\n",
    "    if random.random() < 0.5:\n",
    "        shear_factor = random.uniform(-0.05, 0.05)\n",
    "        M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "    # Random Gaussian Blur\n",
    "    if random.random() < 0.25:\n",
    "        sigma = random.uniform(0, 1.0)\n",
    "        ksize = int(2 * int(sigma * 3 + 0.5) + 1) # Kernel size based on sigma\n",
    "        ksize = ksize if ksize % 2 == 1 else ksize + 1 # Ensure kernel size is odd\n",
    "        if ksize > 1: # Apply blur only if kernel size is greater than 1\n",
    "          augmented_image = cv2.GaussianBlur(augmented_image, (ksize, ksize), sigma)\n",
    "\n",
    "\n",
    "    # Random Gaussian Noise\n",
    "    if random.random() < 0.25:\n",
    "        mean = 0\n",
    "        stddev = random.uniform(0, 0.05 * 255)\n",
    "        noise = np.random.normal(mean, stddev, augmented_image.shape).astype(np.uint8)\n",
    "        augmented_image = cv2.add(augmented_image, noise)\n",
    "\n",
    "\n",
    "    # Random Brightness and Contrast\n",
    "    if random.random() < 0.25:\n",
    "        alpha = random.uniform(0.9, 1.1) # Contrast control (1.0-base contrast)\n",
    "        beta = random.uniform(-10, 10) # Brightness control (0-base brightness)\n",
    "        augmented_image = cv2.convertScaleAbs(augmented_image, alpha=alpha, beta=beta)\n",
    "\n",
    "    # Random Grayscale (only if image is not already grayscale)\n",
    "    if random.random() < 0.25 and len(augmented_image.shape) == 3:\n",
    "         augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    return augmented_image.astype(np.uint8)\n",
    "\n",
    "print(\"Data augmentation function defined using OpenCV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Redefine the augment_image_cv2 function to handle the type mismatch\n",
    "def augment_image_cv2(image):\n",
    "    \"\"\"\n",
    "    Applies a series of random data augmentation techniques to an image using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        image: The input image (NumPy array) with pixel values in [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        The augmented image (NumPy array) with pixel values in [0, 1].\n",
    "    \"\"\"\n",
    "    # Ensure the image is float32 for calculations\n",
    "    augmented_image = image.copy().astype(np.float32)\n",
    "    h, w = augmented_image.shape[:2]\n",
    "\n",
    "    # Random Rotation\n",
    "    if random.random() < 0.5:\n",
    "        angle = random.uniform(-5, 5)\n",
    "        M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Translation\n",
    "    if random.random() < 0.5:\n",
    "        tx = random.uniform(-0.05, 0.05) * w\n",
    "        ty = random.uniform(-0.05, 0.05) * h\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Random Scaling\n",
    "    if random.random() < 0.5:\n",
    "        sx = random.uniform(0.95, 1.05)\n",
    "        sy = random.uniform(0.95, 1.05)\n",
    "        M = np.float32([[sx, 0, 0], [0, sy, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "    # Random Shear (simplified)\n",
    "    if random.random() < 0.5:\n",
    "        shear_factor = random.uniform(-0.05, 0.05)\n",
    "        M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n",
    "        augmented_image = cv2.warpAffine(augmented_image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\n",
    "    # Random Gaussian Blur\n",
    "    if random.random() < 0.25:\n",
    "        sigma = random.uniform(0, 1.0)\n",
    "        ksize = int(2 * int(sigma * 3 + 0.5) + 1) # Kernel size based on sigma\n",
    "        ksize = ksize if ksize % 2 == 1 else ksize + 1 # Ensure kernel size is odd\n",
    "        if ksize > 1: # Apply blur only if kernel size is greater than 1\n",
    "          # GaussianBlur expects image to be uint8 or float32. Since we are already float32, it's fine.\n",
    "          augmented_image = cv2.GaussianBlur(augmented_image, (ksize, ksize), sigma)\n",
    "\n",
    "\n",
    "    # Random Gaussian Noise\n",
    "    if random.random() < 0.25:\n",
    "        mean = 0\n",
    "        # Generate noise with the same dtype as the image\n",
    "        stddev = random.uniform(0, 0.05) # Standard deviation relative to the [0, 1] range\n",
    "        noise = np.random.normal(mean, stddev, augmented_image.shape).astype(np.float32)\n",
    "        # Add noise. Since both inputs are float32, cv2.add will return float32.\n",
    "        augmented_image = cv2.add(augmented_image, noise)\n",
    "        # Clip values to stay in the [0, 1] range\n",
    "        augmented_image = np.clip(augmented_image, 0.0, 1.0)\n",
    "\n",
    "\n",
    "    # Random Brightness and Contrast\n",
    "    if random.random() < 0.25:\n",
    "        alpha = random.uniform(0.9, 1.1) # Contrast control (1.0-base contrast)\n",
    "        beta = random.uniform(-0.1, 0.1) # Brightness control (0-base brightness relative to [0,1])\n",
    "        # Apply brightness and contrast. Since image is float32, beta should be in [0, 1] range.\n",
    "        augmented_image = cv2.convertScaleAbs(augmented_image, alpha=alpha, beta=beta * 255.0) # convertScaleAbs outputs uint8, need to convert back to float32\n",
    "        augmented_image = augmented_image.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "    # Random Grayscale (only if image is not already grayscale)\n",
    "    if random.random() < 0.25 and len(augmented_image.shape) == 3:\n",
    "         augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    return augmented_image\n",
    "\n",
    "# 1. Create empty lists\n",
    "processed_images = []\n",
    "processed_labels = []\n",
    "\n",
    "# Correct Base directory for images\n",
    "base_image_dir = 'extracted_archive/IIIT5K-Word_V3.0/IIIT5K'\n",
    "\n",
    "# Check if df exists and is not empty before proceeding\n",
    "if 'df' in locals() and not df.empty:\n",
    "    # 2. Iterate through each row of the df DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # 3. Get image path and text label\n",
    "        # Ensure column names match your DataFrame\n",
    "        image_path_relative = row['ImgName']\n",
    "        text_label = row['GroundTruth']\n",
    "\n",
    "        # Construct the full image path\n",
    "        full_image_path = os.path.join(base_image_dir, image_path_relative)\n",
    "\n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(full_image_path):\n",
    "            # Read the image\n",
    "            img = cv2.imread(full_image_path)\n",
    "\n",
    "            # If the image is successfully loaded (not None)\n",
    "            if img is not None:\n",
    "                # Apply preprocessing steps\n",
    "                gray_img = grayscale_image(img)\n",
    "                resized_img = resize_image(gray_img, (128, 32))\n",
    "                normalized_img = normalize_pixels(resized_img) # This is float32\n",
    "\n",
    "                # Apply data augmentation\n",
    "                augmented_img = augment_image_cv2(normalized_img) # This function now returns float32\n",
    "\n",
    "                # Append the augmented image (float32) to the processed_images list\n",
    "                processed_images.append(augmented_img)\n",
    "\n",
    "                # Append the text label\n",
    "                processed_labels.append(text_label)\n",
    "            else:\n",
    "                print(f\"Could not load image file: {full_image_path}\")\n",
    "        else:\n",
    "            print(f\"Image file not found: {full_image_path}\")\n",
    "\n",
    "    # 13. Convert the processed_images list into a NumPy array\n",
    "    processed_images = np.array(processed_images)\n",
    "\n",
    "    # 14. Convert the processed_labels list into a Pandas Series or NumPy array\n",
    "    processed_labels = pd.Series(processed_labels)\n",
    "\n",
    "    # 15. Print the shape of the processed_images array and the length of the processed_labels list\n",
    "    print(\"Shape of processed_images array:\", processed_images.shape)\n",
    "    print(\"Length of processed_labels list:\", len(processed_labels))\n",
    "else:\n",
    "    print(\"DataFrame 'df' not found or is empty. Please run the cell to load the CSV first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume resize_image, normalize_pixels, grayscale_image, and augment_image_cv2 are already defined\n",
    "\n",
    "def load_and_preprocess_image(image_path_relative, label, base_image_dir='extracted_archive/IIIT5K-Word_V3.0/IIIT5K'):\n",
    "    \"\"\"\n",
    "    Loads an image, applies preprocessing and data augmentation, and returns the processed image and label.\n",
    "\n",
    "    Args:\n",
    "        image_path_relative: The relative path to the image file from the base_image_dir.\n",
    "        label: The text label for the image.\n",
    "        base_image_dir: The base directory where images are stored.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - processed_image: The processed and augmented image (NumPy array, float32, [0, 1]).\n",
    "            - original_label: The original text label (string).\n",
    "            Or (None, label) if the image file is not found or cannot be loaded.\n",
    "    \"\"\"\n",
    "    full_image_path = os.path.join(base_image_dir, image_path_relative)\n",
    "\n",
    "    if os.path.exists(full_image_path):\n",
    "        img = cv2.imread(full_image_path)\n",
    "\n",
    "        if img is not None:\n",
    "            # Apply preprocessing steps\n",
    "            gray_img = grayscale_image(img)\n",
    "            resized_img = resize_image(gray_img, (128, 32))\n",
    "            normalized_img = normalize_pixels(resized_img) # This is float32\n",
    "\n",
    "            # Apply data augmentation\n",
    "            augmented_img = augment_image_cv2(normalized_img) # This function now returns float32\n",
    "\n",
    "            return augmented_img, label\n",
    "        else:\n",
    "            print(f\"Could not load image file: {full_image_path}\")\n",
    "            return None, label\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")\n",
    "        return None, label\n",
    "\n",
    "# Demonstrate the integration by processing a subset of the DataFrame\n",
    "# Check if df exists and is not empty before proceeding\n",
    "if 'df' in locals() and not df.empty:\n",
    "    subset_df = df.head(5) # Process the first 5 rows for demonstration\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for index, row in subset_df.iterrows():\n",
    "        # Ensure column names match your DataFrame\n",
    "        image_path_relative = row['ImgName']\n",
    "        text_label = row['GroundTruth']\n",
    "\n",
    "        processed_image, original_label = load_and_preprocess_image(image_path_relative, text_label)\n",
    "\n",
    "        if processed_image is not None:\n",
    "            processed_data.append({'processed_image': processed_image, 'text_label': original_label})\n",
    "\n",
    "    # You can now work with the processed_data list\n",
    "    print(f\"Processed {len(processed_data)} images from the subset.\")\n",
    "\n",
    "    # Example: Display the shape of the first processed image\n",
    "    if processed_data:\n",
    "        print(\"Shape of the first processed image:\", processed_data[0]['processed_image'].shape)\n",
    "else:\n",
    "    print(\"DataFrame 'df' not found or is empty. Please run the cell to load the CSV first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers # Import regularizers\n",
    "\n",
    "def build_cnn_backbone(input_shape):\n",
    "    \"\"\"\n",
    "    Builds the CNN backbone for extracting visual features from images, with Dropout regularization.\n",
    "\n",
    "    Args:\n",
    "        input_shape: The shape of the input images (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras Model representing the CNN backbone with regularization.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Initial layers with increased filters and slightly reduced dropout\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2)) # Reduced Dropout\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.2)) # Reduced Dropout\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 1)))\n",
    "    model.add(layers.Dropout(0.25)) # Reduced Dropout\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 1)))\n",
    "    model.add(layers.Dropout(0.25)) # Reduced Dropout\n",
    "\n",
    "    # Added an additional Convolutional layer\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Dropout(0.25)) # Added Dropout after the new conv layer\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(512, (2, 2), activation='relu', padding='valid')) # Smaller kernel for final layer\n",
    "    model.add(layers.Dropout(0.3)) # Adjusted Dropout after final conv layer\n",
    "\n",
    "\n",
    "    # No Flatten or Dense layers as the output is a feature map for the RNN\n",
    "    return model\n",
    "\n",
    "def build_rnn_head(cnn_output_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds the RNN head for interpreting the feature sequence from the CNN, with Dropout regularization.\n",
    "\n",
    "    Args:\n",
    "        cnn_output_shape: The shape of the output feature map from the CNN (height, width, channels).\n",
    "        num_classes: The number of possible characters (including blank).\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras Model representing the RNN head with regularization.\n",
    "    \"\"\"\n",
    "    input_tensor = layers.Input(shape=cnn_output_shape)\n",
    "\n",
    "    # Reshape the input from CNN output (batch, height, width, channels)\n",
    "    # to a sequence format for RNN (batch, time steps, features)\n",
    "    # The time steps will be the width of the feature map, and features will be height * channels\n",
    "    height, width, channels = cnn_output_shape\n",
    "    # Reshape to (batch, width, height * channels)\n",
    "    reshape_layer = layers.Reshape(target_shape=(width, height * channels))(input_tensor)\n",
    "\n",
    "    # Add Bidirectional LSTM layers with increased units and reduced recurrent dropout\n",
    "    # Increased LSTM units to 512\n",
    "    rnn_layer_1 = layers.Bidirectional(layers.LSTM(512, return_sequences=True, recurrent_dropout=0.15))(reshape_layer) # Reduced recurrent_dropout\n",
    "    rnn_layer_1 = layers.Dropout(0.25)(rnn_layer_1) # Reduced Dropout after the first LSTM layer\n",
    "\n",
    "    rnn_layer_2 = layers.Bidirectional(layers.LSTM(512, return_sequences=True, recurrent_dropout=0.15))(rnn_layer_1) # Increased LSTM units, reduced recurrent_dropout\n",
    "    rnn_layer_2 = layers.Dropout(0.25)(rnn_layer_2) # Reduced Dropout after the second LSTM layer\n",
    "\n",
    "    # Added an additional Bidirectional LSTM layer\n",
    "    rnn_layer_3 = layers.Bidirectional(layers.LSTM(512, return_sequences=True, recurrent_dropout=0.15))(rnn_layer_2) # Added LSTM layer\n",
    "    rnn_layer_3 = layers.Dropout(0.25)(rnn_layer_3) # Added Dropout after the new LSTM layer\n",
    "\n",
    "\n",
    "    # Add a Dense layer for character prediction\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(rnn_layer_3)\n",
    "\n",
    "    # Create the RNN model\n",
    "    rnn_model = Model(inputs=input_tensor, outputs=output_layer)\n",
    "\n",
    "    return rnn_model\n",
    "\n",
    "print(\"CNN backbone and RNN head functions redefined with increased capacity and reduced Dropout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf # Import tensorflow to print tensor shape\n",
    "\n",
    "def build_rnn_head(reshaped_cnn_output_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds the RNN head for interpreting the feature sequence from the CNN, with Dropout regularization.\n",
    "\n",
    "    Args:\n",
    "        reshaped_cnn_output_shape: The shape of the reshaped CNN output (time_steps, features).\n",
    "        num_classes: The number of possible characters (including blank).\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras Model representing the RNN head with regularization.\n",
    "    \"\"\"\n",
    "    # Input shape is the shape of the reshaped CNN output, excluding the batch dimension\n",
    "    input_tensor = layers.Input(shape=reshaped_cnn_output_shape)\n",
    "\n",
    "    # The input_tensor is already in the sequence format for RNN (batch, time_steps, features)\n",
    "    # We just need to define the RNN layers on top of this input_tensor.\n",
    "\n",
    "    # Print the shape of the input tensor to the RNN head\n",
    "    print(\"Input shape to RNN head:\", input_tensor.shape)\n",
    "\n",
    "    # Add Bidirectional LSTM layers with increased units and recurrent dropout\n",
    "    # Added recurrent_dropout for regularization within the LSTM cells\n",
    "    rnn_layer_1 = layers.Bidirectional(layers.LSTM(256, return_sequences=True, recurrent_dropout=0.2))(input_tensor)\n",
    "    rnn_layer_1 = layers.Dropout(0.3)(rnn_layer_1) # Added Dropout after the first LSTM layer\n",
    "\n",
    "    rnn_layer_2 = layers.Bidirectional(layers.LSTM(256, return_sequences=True, recurrent_dropout=0.2))(rnn_layer_1)\n",
    "    rnn_layer_2 = layers.Dropout(0.3)(rnn_layer_2) # Added Dropout after the second LSTM layer\n",
    "\n",
    "\n",
    "    # Add a Dense layer for character prediction\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(rnn_layer_2)\n",
    "\n",
    "    # Create the RNN model\n",
    "    rnn_model = Model(inputs=input_tensor, outputs=output_layer)\n",
    "\n",
    "    return rnn_model\n",
    "\n",
    "# Now, rebuild the combined model using the corrected build_rnn_head function\n",
    "\n",
    "# 1. Define the input shape for the overall model\n",
    "# Assuming preprocessed images are grayscale (1 channel), height 32, width 128\n",
    "input_shape = (32, 128, 1)\n",
    "\n",
    "# Define the number of classes (characters + blank)\n",
    "# Using the num_classes from cell a1b65cae which is 70.\n",
    "try:\n",
    "    num_classes = len(char_to_int) # Use the actual number of classes from the character set\n",
    "    print(f\"Using num_classes from data preparation: {num_classes}\")\n",
    "except NameError:\n",
    "    print(\"Warning: 'char_to_int' not found. Using a default num_classes = 70. Please run data preparation cell first.\")\n",
    "    num_classes = 70 # Default value if char_to_int is not defined\n",
    "\n",
    "\n",
    "# 2. Use the previously defined build_cnn_backbone function (with regularization)\n",
    "cnn_backbone = build_cnn_backbone(input_shape)\n",
    "\n",
    "# 3. Define the combined model using the Functional API\n",
    "input_layer = keras.Input(shape=input_shape, name='image_input')\n",
    "\n",
    "# Pass the input through the CNN backbone\n",
    "cnn_output = cnn_backbone(input_layer)\n",
    "\n",
    "# Determine the output shape of the CNN backbone using .shape (static shape)\n",
    "# Exclude batch dimension (index 0)\n",
    "_, height, width, channels = cnn_output.shape\n",
    "\n",
    "# Reshape the CNN output for the RNN\n",
    "# The time steps will be the width of the feature map, and features will be height * channels\n",
    "reshape_output = layers.Reshape(target_shape=(width, height * channels))(cnn_output)\n",
    "\n",
    "# Get the shape of the reshaped output (excluding the batch dimension)\n",
    "reshaped_output_shape = reshape_output.shape[1:]\n",
    "\n",
    "# Use the corrected build_rnn_head function (with regularization)\n",
    "rnn_head = build_rnn_head(reshaped_output_shape, num_classes) # Pass the correct shape\n",
    "\n",
    "# Pass the reshaped output through the RNN head\n",
    "final_output = rnn_head(reshape_output)\n",
    "\n",
    "\n",
    "# Define the final combined model\n",
    "combined_model = keras.Model(inputs=input_layer, outputs=final_output, name='ocr_model_regularized')\n",
    "\n",
    "print(\"Combined CNN-RNN model recreated with corrected RNN input shape and regularization.\")\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd # Assuming df is available from previous steps\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split\n",
    "\n",
    "# 1. Create a character set (vocabulary)\n",
    "# You should build this character set based on the unique characters present in your training labels.\n",
    "# For demonstration, let's use lowercase letters, digits, and a few symbols.\n",
    "# Ensure df is available and filtered to 'train/' images before building vocabulary\n",
    "if 'df' in locals() and not df.empty:\n",
    "    # Build character set from the GroundTruth of the filtered training data\n",
    "    all_ground_truth = \"\".join(df['GroundTruth'].str.lower().tolist())\n",
    "    characters = sorted(list(set(all_ground_truth)))\n",
    "\n",
    "    # Add a blank character for CTC loss\n",
    "    # The blank character is typically the last character in the vocabulary\n",
    "    blank_token = '-'\n",
    "    if blank_token not in characters:\n",
    "        characters.append(blank_token)\n",
    "\n",
    "    # Create a character-to-integer mapping\n",
    "    char_to_int = {char: i for i, char in enumerate(characters)}\n",
    "\n",
    "    # Create an integer-to-character mapping\n",
    "    int_to_char = {i: char for char, i in char_to_int.items()}\n",
    "\n",
    "    # Define the number of classes (including the blank token)\n",
    "    num_classes = len(characters)\n",
    "    print(f\"Number of classes (including blank): {num_classes}\")\n",
    "    print(f\"Character set: {''.join(characters)}\")\n",
    "\n",
    "    # 2. Encode text labels into sequences of integers and store original lengths\n",
    "    def encode_text_labels_with_lengths(labels, char_to_int):\n",
    "        \"\"\"\n",
    "        Encodes a list of text labels into sequences of integers and returns original lengths.\n",
    "\n",
    "        Args:\n",
    "            labels: A list or pandas Series of text labels.\n",
    "            char_to_int: A dictionary mapping characters to integers.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - encoded_labels: A list of integer sequences.\n",
    "                - original_lengths: A list of original label lengths.\n",
    "        \"\"\"\n",
    "        encoded_labels = []\n",
    "        original_lengths = []\n",
    "        for label in labels:\n",
    "            # Ensure label is a string and handle potential NaNs if any\n",
    "            if isinstance(label, str):\n",
    "                encoded_sequence = [char_to_int.get(char, char_to_int[blank_token]) for char in label.lower()] # Convert to lowercase and handle unknown chars\n",
    "                encoded_labels.append(encoded_sequence)\n",
    "                original_lengths.append(len(encoded_sequence))\n",
    "            else:\n",
    "                # Handle non-string labels (e.g., NaNs) - skip or encode as empty\n",
    "                print(f\"Warning: Skipping non-string label: {label}\")\n",
    "                encoded_labels.append([])\n",
    "                original_lengths.append(0)\n",
    "\n",
    "        return encoded_labels, original_lengths\n",
    "\n",
    "    # Encode the text labels from your DataFrame and get original lengths\n",
    "    # Assuming processed_labels is a pandas Series from previous preprocessing steps\n",
    "    # processed_images and processed_labels should correspond to the filtered df\n",
    "    # Re-run preprocessing on the filtered df to ensure processed_images and processed_labels match the filtered df\n",
    "    # For now, let's assume processed_images and processed_labels are already updated to match the filtered df\n",
    "    # (This would require re-running the preprocessing cell after filtering df)\n",
    "\n",
    "    # Let's re-generate processed_images and processed_labels based on the filtered df\n",
    "    # This part should ideally be in the preprocessing cell, but for this fix, we'll do it here.\n",
    "    # In a real pipeline, ensure preprocessing is applied to the filtered df.\n",
    "\n",
    "    # --- Re-generating processed_images and processed_labels from filtered df ---\n",
    "    # Assume grayscale_image, resize_image, normalize_pixels, augment_image_cv2 are defined\n",
    "    # Assume base_image_dir and target_size are defined\n",
    "\n",
    "    # Correct Base directory for images\n",
    "    base_image_dir = 'extracted_archive/IIIT5K-Word_V3.0/IIIT5K'\n",
    "    target_size = (128, 32) # (width, height)\n",
    "\n",
    "    temp_processed_images = []\n",
    "    temp_processed_labels = []\n",
    "\n",
    "    print(\"Re-processing images and labels from filtered training data...\")\n",
    "    for index, row in df.iterrows(): # Use the filtered df\n",
    "        image_path_relative = row['ImgName']\n",
    "        text_label = row['GroundTruth']\n",
    "        full_image_path = os.path.join(base_image_dir, image_path_relative)\n",
    "\n",
    "        if os.path.exists(full_image_path):\n",
    "            img = cv2.imread(full_image_path)\n",
    "            if img is not None:\n",
    "                gray_img = grayscale_image(img)\n",
    "                resized_img = resize_image(gray_img, target_size)\n",
    "                normalized_img = normalize_pixels(resized_img) # float32\n",
    "                # Decide whether to apply augmentation here or later in the tf.data pipeline\n",
    "                # Applying here means augmentation is fixed per epoch. Applying in tf.data is dynamic.\n",
    "                # For simplicity in this fix, let's assume augmentation is applied here as before.\n",
    "                augmented_img = augment_image_cv2(normalized_img) # Assuming augment_image_cv2 is defined\n",
    "\n",
    "                temp_processed_images.append(np.expand_dims(augmented_img, axis=-1)) # Add channel dimension\n",
    "                temp_processed_labels.append(text_label)\n",
    "            else:\n",
    "                print(f\"Warning: Could not load image file during re-processing: {full_image_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found during re-processing: {full_image_path}\")\n",
    "\n",
    "    processed_images = np.array(temp_processed_images)\n",
    "    processed_labels = pd.Series(temp_processed_labels) # Keep as Series for consistency\n",
    "    print(\"Re-processing complete.\")\n",
    "    print(\"Shape of re-processed_images array:\", processed_images.shape)\n",
    "    print(\"Length of re-processed_labels list:\", len(processed_labels))\n",
    "    # --- End Re-generating ---\n",
    "\n",
    "\n",
    "    encoded_labels, original_label_lengths = encode_text_labels_with_lengths(processed_labels, char_to_int)\n",
    "\n",
    "    # Determine the maximum label length\n",
    "    # Use the maximum length from the original encoded labels before padding\n",
    "    max_label_length = max(original_label_lengths) if original_label_lengths else 0\n",
    "    print(f\"Maximum label length: {max_label_length}\")\n",
    "\n",
    "    # 3. Pad the encoded sequences to a fixed length\n",
    "    padded_encoded_labels = pad_sequences(encoded_labels, maxlen=max_label_length, padding='post', value=char_to_int[blank_token])\n",
    "\n",
    "\n",
    "    # --- Split Data into Training and Validation Sets ---\n",
    "    # Use train_test_split from scikit-learn\n",
    "    # Split processed_images, padded_encoded_labels, and original_label_lengths\n",
    "    train_images, val_images, \\\n",
    "    train_labels_padded, val_labels_padded, \\\n",
    "    train_original_lengths, val_original_lengths = train_test_split(\n",
    "        processed_images,\n",
    "        padded_encoded_labels,\n",
    "        original_label_lengths, # Split the list of original lengths\n",
    "        test_size=0.2, # 20% for validation\n",
    "        random_state=42 # for reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set size: {len(train_images)}\")\n",
    "    print(f\"Validation set size: {len(val_images)}\")\n",
    "\n",
    "    # 4. Create TensorFlow Datasets for Training and Validation\n",
    "    # Convert NumPy arrays to TensorFlow Tensors\n",
    "    train_images_tensor = tf.constant(train_images, dtype=tf.float32)\n",
    "    train_labels_tensor = tf.constant(train_labels_padded, dtype=tf.int32)\n",
    "    train_original_lengths_tensor = tf.constant(train_original_lengths, dtype=tf.int32) # Convert list to Tensor\n",
    "\n",
    "    val_images_tensor = tf.constant(val_images, dtype=tf.float32)\n",
    "    val_labels_tensor = tf.constant(val_labels_padded, dtype=tf.int32)\n",
    "    val_original_lengths_tensor = tf.constant(val_original_lengths, dtype=tf.int32) # Convert list to Tensor\n",
    "\n",
    "\n",
    "    # Create datasets\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images_tensor, train_labels_tensor, train_original_lengths_tensor))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images_tensor, val_labels_tensor, val_original_lengths_tensor))\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE) # No need to shuffle validation data\n",
    "\n",
    "    print(\"\\nData prepared for training and validation, and batched.\")\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame 'df' not found or is empty. Please run the cell to load the CSV first and ensure it's filtered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# @tf.function # Use tf.function for better performance\n",
    "def train_step(images, labels, original_lengths, model, optimizer, char_to_int):\n",
    "    \"\"\"\n",
    "    Performs one training step, including forward pass, CTC loss calculation,\n",
    "    and gradient application.\n",
    "\n",
    "    Args:\n",
    "        images: A batch of preprocessed images (TensorFlow Tensor, float32).\n",
    "        labels: A batch of padded integer-encoded labels (TensorFlow Tensor, int32).\n",
    "        original_lengths: A batch of original, unpadded label lengths (TensorFlow Tensor, int32).\n",
    "        model: The combined CNN-RNN model.\n",
    "        optimizer: The optimizer for updating model weights.\n",
    "        char_to_int: Dictionary mapping characters to integers (used to get blank index).\n",
    "\n",
    "    Returns:\n",
    "        The calculated CTC loss for the batch (TensorFlow Tensor).\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: Get model predictions\n",
    "        predictions = model(images, training=True) # training=True is important for layers like BatchNorm, Dropout\n",
    "\n",
    "        # --- Debugging Prints ---\n",
    "        # print(\"\\n--- Debugging train_step inputs/predictions ---\")\n",
    "        # print(\"Images shape:\", images.shape)\n",
    "        # print(\"Labels shape:\", labels.shape)\n",
    "        # print(\"Original lengths shape:\", original_lengths.shape)\n",
    "        # print(\"Predictions shape:\", predictions.shape)\n",
    "        # print(\"Predictions dtype:\", predictions.dtype)\n",
    "        # print(\"Predictions min value:\", tf.reduce_min(predictions))\n",
    "        # print(\"Predictions max value:\", tf.reduce_max(predictions))\n",
    "        # print(\"Predictions mean value:\", tf.reduce_mean(predictions))\n",
    "        # print(\"Predictions contains NaNs:\", tf.reduce_any(tf.math.is_nan(predictions)))\n",
    "        # print(\"Predictions contains Infs:\", tf.reduce_any(tf.math.is_inf(predictions)))\n",
    "        # # print(\"First few predictions (softmax output):\", predictions[0, :5, :].numpy()) # Optional: print actual values\n",
    "        # print(\"--- End Debugging ---\")\n",
    "\n",
    "\n",
    "        # Calculate CTC Loss using tf.compat.v1.nn.ctc_loss\n",
    "        # tf.compat.v1.nn.ctc_loss expects log probabilities\n",
    "        predictions_log_softmax = tf.math.log(tf.clip_by_value(predictions, 1e-8, 1.0))\n",
    "\n",
    "        # Create SparseTensor for true labels using original lengths\n",
    "        blank_index = char_to_int['-'] # Get the index of the blank token\n",
    "        # Find the indices of non-blank characters\n",
    "        indices = tf.where(tf.not_equal(labels, blank_index))\n",
    "        # Get the values at these indices\n",
    "        values = tf.gather_nd(labels, indices)\n",
    "        # Get the dense shape (batch_size, max_label_length) - use max_label_length from data prep\n",
    "        # The dense shape needs to be consistent with the padded labels\n",
    "        dense_shape = tf.shape(labels, out_type=tf.int64) # Use shape of padded labels\n",
    "\n",
    "        # Create the SparseTensor\n",
    "        sparse_labels_tensor = tf.SparseTensor(indices, values, dense_shape)\n",
    "\n",
    "\n",
    "        # --- Debugging SparseTensor ---\n",
    "        # print(\"\\n--- Debugging SparseTensor for labels ---\")\n",
    "        # print(\"SparseTensor indices shape:\", tf.shape(indices))\n",
    "        # print(\"SparseTensor values shape:\", tf.shape(values))\n",
    "        # print(\"SparseTensor dense shape:\", dense_shape)\n",
    "        # # print(\"First few SparseTensor indices:\", indices[:10].numpy()) # Optional\n",
    "        # # print(\"First few SparseTensor values:\", values[:10].numpy()) # Optional\n",
    "        # print(\"--- End Debugging ---\")\n",
    "\n",
    "\n",
    "        # Get input lengths (time steps from predictions)\n",
    "        # input_length = tf.fill([tf.shape(predictions)[0]], tf.shape(predictions)[1]) # This might be too large if padding is excessive\n",
    "        # Use the actual number of time steps available for prediction, which is the width of the RNN output\n",
    "        input_length = tf.fill([tf.shape(predictions)[0]], tf.shape(predictions)[1]) # Number of time steps is the second dimension of predictions\n",
    "\n",
    "        # --- Debugging Input Length ---\n",
    "        # print(\"\\n--- Debugging Input Length for CTC ---\")\n",
    "        # print(\"Input length shape:\", input_length.shape)\n",
    "        # print(\"Input length values (first 5):\", input_length[:5].numpy()) # Should be constant within a batch\n",
    "        # print(\"--- End Debugging ---\")\n",
    "\n",
    "\n",
    "        # Calculate the CTC loss\n",
    "        # Pass the original_lengths_batch as sequence_length (experimental fix)\n",
    "        loss = tf.compat.v1.nn.ctc_loss(\n",
    "            labels=tf.cast(sparse_labels_tensor, tf.int32), # Use the correctly named SparseTensor here\n",
    "            inputs=predictions_log_softmax,\n",
    "            sequence_length=input_length, # Using input_length derived from predictions shape\n",
    "            time_major=False\n",
    "        )\n",
    "\n",
    "        # tf.compat.v1.nn.ctc_loss returns per-batch loss, take the mean\n",
    "        mean_batch_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(mean_batch_loss, model.trainable_variables)\n",
    "\n",
    "    # Apply gradient clipping\n",
    "    # Clip gradients by global norm to prevent exploding gradients\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=5.0) # You can adjust clip_norm value\n",
    "\n",
    "\n",
    "    # --- Debugging Gradients ---\n",
    "    # print(\"\\n--- Debugging Gradients ---\")\n",
    "    # for grad, var in zip(clipped_gradients, model.trainable_variables):\n",
    "    #     if grad is not None:\n",
    "    #         print(f\"Gradient for {var.name}: shape={grad.shape}, min={tf.reduce_min(grad):.4f}, max={tf.reduce_max(grad):.4f}, mean={tf.reduce_mean(grad):.4f}\")\n",
    "    #     else:\n",
    "    #         print(f\"Gradient for {var.name}: None\")\n",
    "    # print(\"--- End Debugging ---\")\n",
    "\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return mean_batch_loss\n",
    "\n",
    "print(\"Debugging prints for gradients commented out in train_step function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Re-instantiate the optimizer if needed (optional, but ensures a fresh start)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with the Adam optimizer and a dummy loss function.\n",
    "# The actual CTC loss calculation will be done in the custom training loop/step.\n",
    "combined_model.compile(optimizer=optimizer, loss=None)\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer (learning rate 0.001) and dummy loss.\")\n",
    "\n",
    "# --- Manual Callback Implementation Setup ---\n",
    "# Initialize variables for Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_since_last_improvement = 0\n",
    "best_weights = None # To store the best model weights\n",
    "\n",
    "# Initialize variables for ReduceLROnPlateau\n",
    "lr_patience_counter = 0\n",
    "current_lr = tf.keras.backend.get_value(optimizer.learning_rate)\n",
    "\n",
    "# Get the number of batches for training and validation (recalculate in case dataset changed)\n",
    "num_train_batches = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_val_batches = tf.data.experimental.cardinality(val_dataset).numpy()\n",
    "print(f\"Number of training batches per epoch: {num_train_batches}\")\n",
    "print(f\"Number of validation batches per epoch: {num_val_batches}\")\n",
    "\n",
    "\n",
    "# Define the number of epochs for training\n",
    "epochs = 70 # Increased epochs to give the model more time to learn\n",
    "\n",
    "# --- Manual Callback Patientce Adjustment ---\n",
    "# Increased patience for Early Stopping as requested\n",
    "early_stopping_patience = 10 # Increased patience from 5 to 10\n",
    "\n",
    "# Keep ReduceLROnPlateau patience as is for now, can adjust later if needed\n",
    "reduce_lr_patience = 3\n",
    "reduce_lr_factor = 0.5\n",
    "reduce_lr_min_lr = 0.00001\n",
    "\n",
    "# --- Training Loop with Manual Callback Logic ---\n",
    "print(f\"\\nStarting training with manual callback logic for {epochs} epochs...\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training steps\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Training...\")\n",
    "    for step, (images_batch, labels_batch, original_lengths_batch) in enumerate(train_dataset):\n",
    "        # Perform a training step\n",
    "        batch_loss = train_step(images_batch, labels_batch, original_lengths_batch, combined_model, optimizer, char_to_int)\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        epoch_train_loss += batch_loss\n",
    "\n",
    "        # Optional: Print loss periodically\n",
    "        if step % 50 == 0: # Print more frequently to see progress\n",
    "            print(f\"Epoch {epoch+1}, Step {step}/{num_train_batches}: Training Loss = {batch_loss.numpy():.4f}\")\n",
    "\n",
    "\n",
    "    # Calculate average epoch training loss\n",
    "    if num_train_batches > 0:\n",
    "        average_epoch_train_loss = epoch_train_loss / num_train_batches\n",
    "    else:\n",
    "        average_epoch_train_loss = 0.0\n",
    "\n",
    "\n",
    "    # --- Validation Step ---\n",
    "    epoch_val_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Validating...\")\n",
    "    for step, (val_images_batch, val_labels_batch, val_original_lengths_batch) in enumerate(val_dataset):\n",
    "        # Forward pass on validation data (training=False)\n",
    "        val_predictions = combined_model(val_images_batch, training=False)\n",
    "\n",
    "        # Calculate CTC Loss for validation batch\n",
    "        # Using the same logic as in train_step, but without gradients\n",
    "        val_predictions_log_softmax = tf.math.log(tf.clip_by_value(val_predictions, 1e-8, 1.0))\n",
    "\n",
    "        # Create SparseTensor for validation labels\n",
    "        blank_index = char_to_int['-']\n",
    "        indices = tf.where(tf.not_equal(val_labels_batch, blank_index))\n",
    "        values = tf.gather_nd(val_labels_batch, indices)\n",
    "        dense_shape = tf.shape(val_labels_batch, out_type=tf.int64)\n",
    "        sparse_val_labels_tensor = tf.SparseTensor(indices, values, dense_shape)\n",
    "\n",
    "        # Get input lengths (time steps from predictions)\n",
    "        val_input_length = tf.fill([tf.shape(val_predictions)[0]], tf.shape(val_predictions)[1])\n",
    "\n",
    "        val_batch_loss = tf.compat.v1.nn.ctc_loss(\n",
    "            labels=tf.cast(sparse_val_labels_tensor, tf.int32),\n",
    "            inputs=val_predictions_log_softmax,\n",
    "            sequence_length=val_input_length,\n",
    "            time_major=False\n",
    "        )\n",
    "\n",
    "        # Accumulate the validation loss\n",
    "        epoch_val_loss += tf.reduce_mean(val_batch_loss) # Take mean per batch\n",
    "\n",
    "    # Calculate average epoch validation loss\n",
    "    if num_val_batches > 0:\n",
    "        average_epoch_val_loss = epoch_val_loss / num_val_batches\n",
    "    else:\n",
    "        average_epoch_val_loss = 0.0\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished in {epoch_duration:.2f} seconds.\")\n",
    "    print(f\"Average Training Loss: {average_epoch_train_loss.numpy():.4f}\")\n",
    "    print(f\"Average Validation Loss: {average_epoch_val_loss.numpy():.4f}\")\n",
    "\n",
    "\n",
    "    # --- Manual Callback Checks using Validation Loss ---\n",
    "    # Early Stopping Logic\n",
    "    if average_epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = average_epoch_val_loss\n",
    "        epochs_since_last_improvement = 0\n",
    "        # Save best weights manually\n",
    "        best_weights = combined_model.get_weights() # Store the model weights\n",
    "        print(f\"Validation loss improved. Best validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_since_last_improvement += 1\n",
    "        print(f\"Validation loss did not improve. Epochs since last improvement: {epochs_since_last_improvement}/{early_stopping_patience}\")\n",
    "        # Check if early stopping patience is reached\n",
    "        if epochs_since_last_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            # Restore best weights\n",
    "            if best_weights is not None:\n",
    "                combined_model.set_weights(best_weights)\n",
    "                print(\"Restored best model weights.\")\n",
    "            break # Exit the training loop\n",
    "\n",
    "\n",
    "    # ReduceLROnPlateau Logic\n",
    "    if epochs_since_last_improvement > 0: # Only reduce LR if validation loss hasn't improved\n",
    "        lr_patience_counter += 1\n",
    "        if lr_patience_counter >= reduce_lr_patience:\n",
    "            # Reduce learning rate\n",
    "            old_lr = tf.keras.backend.get_value(optimizer.learning_rate)\n",
    "            new_lr = max(old_lr * reduce_lr_factor, reduce_lr_min_lr)\n",
    "            # Corrected line: Directly set the learning_rate attribute\n",
    "            optimizer.learning_rate.assign(new_lr)\n",
    "            print(f\"Reducing learning rate from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "            lr_patience_counter = 0 # Reset counter\n",
    "    else:\n",
    "        lr_patience_counter = 0 # Reset counter if validation loss improved\n",
    "\n",
    "\n",
    "    print(f\"Current Learning Rate: {tf.keras.backend.get_value(optimizer.learning_rate):.6f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define the directory where you want to save the model\n",
    "model_save_dir = 'saved_ocr_model'\n",
    "os.makedirs(model_save_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the path to save the model with a .keras extension\n",
    "model_save_path = os.path.join(model_save_dir, 'combined_ocr_model.keras') # Added .keras extension\n",
    "\n",
    "# Check if the combined_model is defined and trained\n",
    "if 'combined_model' in locals() and isinstance(combined_model, tf.keras.Model):\n",
    "    print(f\"Saving the trained model to: {model_save_path}\")\n",
    "    try:\n",
    "        # Save the model in the native Keras format\n",
    "        combined_model.save(model_save_path)\n",
    "        print(\"Model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "else:\n",
    "    print(\"Error: The 'combined_model' is not defined or is not a valid Keras Model. Please ensure the model has been built and trained.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
